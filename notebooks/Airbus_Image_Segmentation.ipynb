{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.getcwd()\n",
    "os.chdir(r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94604c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan\n",
    "import os\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import numpy as np\n",
    "class config_():\n",
    "    def __init__(self):\n",
    "        self.DATA_DIR = r\"../input\"\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.IMAGE_WIDTH = 320\n",
    "        self.IMAGE_HEIGHT = 320\n",
    "        self.NUM_WORKERS = 2\n",
    "        self.EPOCHS = 1\n",
    "        self.DEVICE = \"cuda\"\n",
    "        self.CROP_SIZE = (self.IMAGE_WIDTH, self.IMAGE_HEIGHT)\n",
    "        self.NUM_CLASSES = 1\n",
    "\n",
    "        self.TRAIN_PATH = os.listdir('../input/train_v2')\n",
    "        self.TEST_PATH = os.listdir('../input/test_v2')\n",
    "        self.MASKS = pd.read_csv('../input/train_ship_segmentations_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11314c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MASKS = config.MASKS\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    if isinstance(mask_rle, float):\n",
    "        return np.zeros((768, 768))\n",
    "    else:\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def read_img(train_files):\n",
    "    images = []\n",
    "    labels = []\n",
    "    path = '../input/train_v2'\n",
    "\n",
    "    for ImageId in train_files:\n",
    "        img = cv2.imread(os.path.join(path, ImageId))\n",
    "        img_masks = MASKS.loc[MASKS['ImageId'] == ImageId, 'EncodedPixels'].tolist()\n",
    "\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768))\n",
    "        for mask in img_masks:\n",
    "            all_masks += rle_decode(mask)\n",
    "            \n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # img = img.astype(\"float32\") / 255.0\n",
    "        img = cv2.resize(img, config.CROP_SIZE)\n",
    "        all_masks = cv2.resize(all_masks, config.CROP_SIZE)\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(all_masks)\n",
    "    return images, labels\n",
    "\n",
    "class AirbusDataset(torch.utils.data.Dataset):\n",
    "    # \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
    "    def __init__(self, train_files, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.features, self.labels = read_img(train_files=train_files)\n",
    "        print('reading ' + str(len(self.features)) + ' examples')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = self.features[idx], self.labels[idx]\n",
    "        if self.transforms is not None:\n",
    "\t\t\t# apply the transformations to both image and its mask\n",
    "            feature = self.transforms(feature)\n",
    "            label = self.transforms(label)\n",
    "        return (feature, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = config.TRAIN_PATH[:5000]\n",
    "test_f = config.TRAIN_PATH[2000:2500]\n",
    "features, labels = read_img(train_f[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feefc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0].shape, labels[0].shape, type(features[0]), type(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image2 = cv2.cvtColor(features[2], cv2.COLOR_BGR2RGB)\n",
    "# image2 = cv2.resize(features[0], (128, 128))\n",
    "# gtMask = cv2.resize(labels[0], (128, 128))\n",
    "# plt.imshow(image2)\n",
    "# # plt.imshow(gtMask)\n",
    "# image2.shape, gtMask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import config\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import torch.nn.functional as F\n",
    "import dataset\n",
    "import model\n",
    "import engine\n",
    "\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "                                 # transforms.ToPILImage(),\n",
    "                                transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_f = config.TRAIN_PATH[:2000]\n",
    "test_f = config.TRAIN_PATH[2000:2500]\n",
    "\n",
    "def loss_fn(inputs, targets):\n",
    "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1) \n",
    "\n",
    "def run_training():\n",
    "    voc_train = dataset.AirbusDataset(train_files=train_f, transforms=transforms)\n",
    "    voc_test = dataset.AirbusDataset(train_files=test_f, transforms=transforms)\n",
    "\n",
    "    train_loader = DataLoader(voc_train, batch_size = config.BATCH_SIZE,num_workers = config.NUM_WORKERS, shuffle = True, drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(voc_test, batch_size = config.BATCH_SIZE,num_workers = config.NUM_WORKERS, shuffle = False, drop_last=True)\n",
    "\n",
    "    # Build Model\n",
    "\n",
    "    net = model.ImageSegmentation(num_classes=config.NUM_CLASSES)\n",
    "\n",
    "    num_epochs, lr, wd, devices = config.EPOCHS, 0.001, 1e-3, d2l.try_all_gpus()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    engine.train_fn(net, train_loader, test_loader, loss_fn, optimizer, num_epochs, devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = dataset.AirbusDataset(train_files=train_f[:50], transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(voc_train, batch_size = 8, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_b, labels_b = next(iter(train_loader))\n",
    "images_b.shape, labels_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b54332",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x) in voc_train.labels:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10972e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "        transforms.Resize(config.CROP_SIZE),\n",
    "        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a802ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def read_img2(train_files):\n",
    "    images = []\n",
    "    labels = []\n",
    "    path = '../input/train_v2'\n",
    "    path2 = r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\input\\train_v2_masks'\n",
    "    \n",
    "    for ImageId in train_files:\n",
    "        img_path = os.path.join(path, ImageId)\n",
    "        img_masks = MASKS.loc[MASKS['ImageId'] == ImageId, 'EncodedPixels'].tolist()\n",
    "\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768))\n",
    "        for mask in img_masks:\n",
    "            all_masks += rle_decode(mask)\n",
    "\n",
    "        I = all_masks\n",
    "        I8 = (((I - I.min()) / (I.max() - I.min())) * 255.9).astype(np.uint8)\n",
    "        mask = Image.fromarray(I8)\n",
    "        if os.path.exists(os.path.join(path2,f'{ImageId}')) == False:\n",
    "            mask.save(os.path.join(path2,f'{ImageId}'))\n",
    "        images.append(img_path)\n",
    "        labels.append(os.path.join(path2,f'{ImageId}'))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDataset(torch.utils.data.Dataset):\n",
    "    # \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
    "    def __init__(self, train_files, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.train_files = train_files\n",
    "        \n",
    "        self.imagePaths, self.maskPaths = read_img2(train_files=self.train_files)\n",
    "        print('reading ' + str(len(self.imagePaths)) + ' examples')\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return the number of total samples contained in the dataset\n",
    "        return len(self.imagePaths)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # grab the image path from the current index\n",
    "        imagePath = self.imagePaths[idx]\n",
    "\n",
    "        # load the image from disk, swap its channels from BGR to RGB,\n",
    "        # and read the associated mask from disk in grayscale mode\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.maskPaths[idx], 0)\n",
    "\n",
    "        # check to see if we are applying any transformations\n",
    "        if self.transforms is not None:\n",
    "            # apply the transformations to both image and its mask\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "\n",
    "        # return a tuple of the image and its mask\n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97610b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = config.TRAIN_PATH[:5000]\n",
    "#test_f = config.TRAIN_PATH[2000:2500]\n",
    "features, labels = read_img2(train_f[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = AirbusDataset(train_files=train_f[:5000], transforms=transforms)\n",
    "train_loader = torch.utils.data.DataLoader(voc_train, batch_size = 5000, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c17a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(voc_train, batch_size = 5000, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d311de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_b, labels_b = next(iter(train_loader))\n",
    "images_b.shape, labels_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.imread(labels[5], 0)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.permute(images_b[7], (2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageSegmentation(num_classes):\n",
    "    PRE_TRAINED_NET = torchvision.models.resnet18(pretrained=True)\n",
    "    model = nn.Sequential(*list(PRE_TRAINED_NET.children())[:-2])\n",
    "    model.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "    model.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "    W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "    model.transpose_conv.weight.data.copy_(W)\n",
    "\n",
    "    return model\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),torch.arange(kernel_size).reshape(1, -1))\n",
    "    \n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "    kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35392f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "path2 = r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\src\\model_1.pt'\n",
    "imagePath = r'D:\\PyTorch\\Airbus_Image_Segmentation\\test_airbus\\00e90efc3.jpg'\n",
    "path = r\"D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\models\\model_1.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "net = ImageSegmentation(1)\n",
    "net.load_state_dict(checkpoint['model'])\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net = net.eval()\n",
    "net = net.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms_image = transforms.Compose([transforms.ToPILImage(),\n",
    "                                 transforms.Resize((768, 768)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 torchvision.transforms.Normalize(\n",
    "                                        mean=[0.5, 0.5, 0.5],\n",
    "                                        std=[0.29, 0.29, 0.29], )\n",
    "                                 ])\n",
    "\n",
    "def make_predictions(model, imagePath):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    config = config_()\n",
    "    # turn off gradient tracking\n",
    "    with torch.no_grad():\n",
    "        # load the image from disk, swap its color channels, cast it\n",
    "        # to float data type, and scale its pixel values\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = image.astype(\"float32\") / 255.0\n",
    "        # resize the image and make a copy of it for visualization\n",
    "        image = transforms_image(image)\n",
    "        image = np.expand_dims(image, 0)\n",
    "        image = torch.Tensor(image)\n",
    "        image = image.to('cuda')\n",
    "        print(image.shape)\n",
    "        # find the filename and generate the path to ground truth\n",
    "        # mask\n",
    "#         filename = imagePath.split(os.path.sep)[-1]\n",
    "#         groundTruthPath = os.path.join(config.MASK_DATASET_PATH,\n",
    "#             filename)\n",
    "#         # load the ground-truth segmentation mask in grayscale mode\n",
    "#         # and resize it\n",
    "#         gtMask = cv2.imread(groundTruthPath, 0)\n",
    "#         gtMask = cv2.resize(gtMask, (320, 320))\n",
    "        return model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20179468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "pred = make_predictions(net, imagePath)\n",
    "k = torch.permute(pred, (0, 2, 3, 1))\n",
    "k = torch.squeeze(k)\n",
    "k = k.cpu() \n",
    "#k = np.uint8(255) - k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread(imagePath )\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "loss2 = nn.BCEWithLogitsLoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output2 = loss2(input, target)\n",
    "print(output, input, target , output2, m(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n",
    "output = torch.full([10, 64], 1.5)  # A prediction (logit)\n",
    "pos_weight = torch.ones([64])  # All weights are equal to 1\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "print(criterion(output, target), target.shape, output.shape)  # -log(sigmoid(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fa407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(output, input.shape, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f162301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "print(output, input.shape, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2 = torch.squeeze(labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e58822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "5000 * 320 * 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positives = 537256.1250\n",
    "num_negatives = 512000000 - num_positives\n",
    "pos_weight  = num_negatives / num_positives\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels), np.sum(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ca341",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [np.sum(x) for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ef2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(k) / 512000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "320 * 320 * 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5c351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (image_seg)",
   "language": "python",
   "name": "image_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
