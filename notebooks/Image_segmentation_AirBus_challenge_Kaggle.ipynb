{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e607fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "from skimage import io, transform\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeba8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage import io, transform\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image \n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae332698",
   "metadata": {},
   "source": [
    "**Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c88da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "data_root = r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\input'\n",
    "path_train = os.path.join(data_root,'train_v2')\n",
    "path_test = os.path.join(data_root,'test_v2')\n",
    "\n",
    "# Booleans\n",
    "SHOW_PIXELS_DIST = False\n",
    "SHOW_SHIP_DIAG = False\n",
    "SHOW_IMG_LOADER = False\n",
    "\n",
    "# Training variables\n",
    "BATCH_SZ_TRAIN = 16\n",
    "BATCH_SZ_VALID = 4\n",
    "LR = 1e-4\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Define loss function\n",
    "LOSS = 'BCEWithDigits' # BCEWithDigits | FocalLossWithDigits | BCEDiceWithLogitsLoss | BCEJaccardWithLogitsLoss\n",
    "\n",
    "# # Define model\n",
    "# MODEL_SEG = 'UNET_RESNET34ImgNet' # UNET | IUNET | UNET_RESNET34ImgNet \n",
    "# FREEZE_RESNET = False   # if UNET_RESNET34ImgNet\n",
    "\n",
    "# # Fetch U-Net with a pre-trained RESNET34 encoder on imagenet\n",
    "# if MODEL_SEG == 'UNET_RESNET34ImgNet':\n",
    "#     !pip install git+https://github.com/qubvel/segmentation_models.pytorch > /dev/null 2>&1 # Install segmentations_models.pytorch, with no bash output.\n",
    "#     import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d6761",
   "metadata": {},
   "source": [
    "**Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n",
    "\n",
    "def imshow_mask(img, mask):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    mask = mask.numpy().transpose((1, 2, 0))\n",
    "    mask = np.clip(mask, 0, 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,30))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(mask)\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "def imshow_gt_out(img, mask_gt, mask_out):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    mask_gt = mask_gt.numpy().transpose((1, 2, 0))\n",
    "    mask_gt = np.clip(mask_gt, 0, 1)\n",
    "\n",
    "    mask_out = mask_out.numpy().transpose((1, 2, 0))\n",
    "    mask_out = np.clip(mask_out, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(10,30))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Input image\")\n",
    "    axs[1].imshow(mask_gt)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\"Ground truth\")\n",
    "    axs[2].imshow(mask_out)\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"Model output\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "def imshow_overlay(img, mask, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    mask = mask.numpy().transpose((1, 2, 0))\n",
    "    mask = np.clip(mask, 0, 1)\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    plt.imshow(mask_overlay(img, mask))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) \n",
    "    \n",
    "def mask_overlay(image, mask, color=(0, 1, 0)):\n",
    "    \"\"\"\n",
    "    Helper function to visualize mask on the top of the image\n",
    "    \"\"\"\n",
    "    mask = np.dstack((mask, mask, mask)) * np.array(color)\n",
    "    weighted_sum = cv2.addWeighted(mask, 0.5, image, 0.5, 0.)\n",
    "    img = image.copy()\n",
    "    ind = mask[:, :, 1] > 0\n",
    "    img[ind] = weighted_sum[ind]    \n",
    "    return img\n",
    "\n",
    "def rle_to_pixels(rle_code):\n",
    "    rle_code = [int(i) for i in rle_code.split()]\n",
    "    pixels = [(pixel_position % 768, pixel_position // 768) \n",
    "                 for start, length in list(zip(rle_code[0:-1:2], rle_code[1:-2:2])) \n",
    "                 for pixel_position in range(start, start + length)]\n",
    "    return pixels\n",
    "\n",
    "def show_pixels_distribution(df):\n",
    "    \"\"\"\n",
    "    Prints the amount of ship and no-ship pixels in the df\n",
    "    \"\"\"\n",
    "    # Total images in the df\n",
    "    n_images = df['ImageId'].nunique() \n",
    "    \n",
    "    # Total pixels in the df\n",
    "    total_pixels = n_images * 768 * 768 \n",
    "\n",
    "    # Keep only rows with RLE boxes, transform them into list of pixels, sum the lengths of those lists\n",
    "    ship_pixels = df['EncodedPixels'].dropna().apply(rle_to_pixels).str.len().sum() \n",
    "\n",
    "    ratio = ship_pixels / total_pixels\n",
    "    print(f\"Ship: {round(ratio, 3)} ({ship_pixels})\")\n",
    "    print(f\"No ship: {round(1 - ratio, 3)} ({total_pixels - ship_pixels})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03084242",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = pd.read_csv(os.path.join(data_root, 'train_ship_segmentations_v2.csv'))\n",
    "print('Total number of images (original): %d' % masks['ImageId'].value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62af7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_PIXELS_DIST == True:\n",
    "    show_pixels_distribution(masks)\n",
    "    show_pixels_distribution(masks.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508db546",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = masks[~masks['ImageId'].isin(['6384c3e78.jpg'])] # remove corrupted file\n",
    "unique_img_ids = masks.groupby('ImageId').size().reset_index(name='counts')\n",
    "print('Total number of images (after removing corrupted images): %d' % masks['ImageId'].value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8188a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_wships = masks[~masks['EncodedPixels'].isna()].sample(9)\n",
    "# fig, arr = plt.subplots(3,3, figsize=(10,10), constrained_layout=True)\n",
    "# for i, img in enumerate(img_wships['ImageId']):\n",
    "#     r = int(i / 3)\n",
    "#     c = i % 3\n",
    "#     arr[r,c].imshow(imread(os.path.join(path_train, img)))\n",
    "#     arr[r,c].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_woships = masks[masks['EncodedPixels'].isna()].sample(9)\n",
    "# fig, arr = plt.subplots(3,3, figsize=(10,10), constrained_layout=True)\n",
    "# for i, img in enumerate(img_woships['ImageId']):\n",
    "#     r = int(i / 3)\n",
    "#     c = i % 3\n",
    "#     arr[r,c].imshow(imread(os.path.join(path_train, img)))\n",
    "#     arr[r,c].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1362a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wships = masks.dropna()\n",
    "df_wships = df_wships.groupby('ImageId').size().reset_index(name='counts')\n",
    "df_woships = masks[masks['EncodedPixels'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccedcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(['With ships','Without ships'], [len(df_wships), len(df_woships)])\n",
    "# plt.ylabel('Number of images')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of images with ships : %d | Number of images without ships : %d  (x%0.1f)' \\\n",
    "#       % (df_wships.shape[0], df_woships.shape[0], df_woships.shape[0] / df_wships.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = masks.dropna()\n",
    "df_woships = masks[masks['EncodedPixels'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff398153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(['With ships','Without ships'], [len(df_wships), len(df_woships)])\n",
    "# plt.ylabel('Number of images')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of images with ships : %d | Number of images without ships : %d  (x%0.1f)' \\\n",
    "#       % (df_wships.shape[0], df_woships.shape[0], df_woships.shape[0] / df_wships.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b903f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_wships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = df_wships.hist(bins=df_wships['counts'].max())\n",
    "# plt.title(\"Histogram of ships count\")\n",
    "# plt.xlabel(\"Number of ships\")\n",
    "# plt.ylabel(\"Number of images\")\n",
    "# plt.show(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w15ships = df_wships.loc[df_wships['counts'] == 5]\n",
    "list_w15ships = df_w15ships.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axarr = plt.subplots(2, 2, figsize=(10, 10), constrained_layout=True)\n",
    "# for i in range(4):\n",
    "#     rd_id = random.randrange(len(list_w15ships))\n",
    "#     img_masks = masks.loc[masks['ImageId'] == str(list_w15ships[rd_id][0]), 'EncodedPixels'].tolist()\n",
    "\n",
    "#     # Take the individual ship masks and create a single mask array for all ships\n",
    "#     all_masks = np.zeros((768, 768))\n",
    "#     for mask in img_masks:\n",
    "#         all_masks += rle_decode(mask)\n",
    "        \n",
    "#     r = int(i / 2)\n",
    "#     c = i % 2\n",
    "\n",
    "#     axarr[r][c].imshow(imread(os.path.join(path_train, list_w15ships[rd_id][0])))\n",
    "#     axarr[r][c].imshow(all_masks, alpha=0.3)\n",
    "#     axarr[r][c].axis('off')\n",
    "                    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_img_ids = masks.groupby('ImageId').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(unique_img_ids, test_size=0.05, stratify=unique_img_ids['counts'], random_state=42)\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df['ImageId']).value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['counts'] = train_df.apply(lambda c_row: c_row['counts'] if isinstance(c_row['EncodedPixels'], str) else 0, 1)\n",
    "valid_df['counts'] = valid_df.apply(lambda c_row: c_row['counts'] if isinstance(c_row['EncodedPixels'], str) else 0, 1)\n",
    "\n",
    "print('Number of training images : %d' % train_df['ImageId'].value_counts().shape[0])\n",
    "train_df['counts'].hist(bins=train_df['counts'].max())\n",
    "plt.title(\"Histogram of ships count (training)\")\n",
    "plt.xlabel(\"Number of ships\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()\n",
    "\n",
    "print('Number of validation images : %d' % valid_df['ImageId'].value_counts().shape[0])\n",
    "valid_df['counts'].hist(bins=valid_df['counts'].max())\n",
    "plt.title(\"Histogram of ships count (validation)\")\n",
    "plt.xlabel(\"Number of ships\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7f06c",
   "metadata": {},
   "source": [
    "**Segmentation masks to bounding boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if SHOW_SHIP_DIAG == True:\n",
    "# # Transform masks to bounding boxes\n",
    "# # Analisys of ship sizes through the dataset\n",
    "\n",
    "# bboxes = []\n",
    "# bboxes_dict = {}\n",
    "\n",
    "# # Compute bouding boxes coordinates\n",
    "# for img_id in tqdm(df_wships['ImageId']):\n",
    "#     bboxes = []\n",
    "\n",
    "#     # Get binary mask\n",
    "#     masks_val = masks.loc[masks['ImageId'] == str(img_id), 'EncodedPixels'].tolist()\n",
    "\n",
    "#     # Take the individual ship masks and create a single mask array for all ships\n",
    "#     bin_mask = np.zeros((768, 768))\n",
    "#     for mask in masks_val:\n",
    "#         bin_mask += rle_decode(mask)\n",
    "\n",
    "#     # Extract bounding boxes\n",
    "#     lbl = label(bin_mask)\n",
    "#     props = regionprops(lbl)\n",
    "\n",
    "#     for prop in props:\n",
    "#         bboxes.append(prop.bbox)\n",
    "\n",
    "#     bboxes_dict[img_id] = bboxes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bboxes_dict_df =  pd.DataFrame.from_dict(bboxes_dict, orient ='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a81e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bboxes_dict_df.to_csv(os.path.join(data_root, 'bboxes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot some images with bouding boxes\n",
    "# fig, axarr = plt.subplots(2, 2, figsize = (15, 15), constrained_layout=True)\n",
    "# for i in range(4):\n",
    "#     img_id = df_wships.loc[i*4, 'ImageId']\n",
    "#     img = imread(os.path.join(path_train, str(img_id)))\n",
    "\n",
    "#     bboxs = bboxes_dict[img_id]\n",
    "#     for bbox in bboxs:\n",
    "#         cv2.rectangle(img, (bbox[1], bbox[0]), (bbox[3], bbox[2]), (255, 0, 0), 2)\n",
    "\n",
    "#     r = int(i / 2)\n",
    "#     c = i % 2\n",
    "#     axarr[r][c].imshow(img)\n",
    "#     axarr[r][c].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b229997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute diagonal of each ships and plot histogram\n",
    "# diag = []\n",
    "# for i in bboxes_dict:\n",
    "#     for j in bboxes_dict[i]:\n",
    "#         diag.append(int(np.sqrt((j[0] - j[2]) ** 2 + (j[1] - j[3]) ** 2)))\n",
    "\n",
    "# df_diag = pd.DataFrame(diag, columns =['Diagonal size'])\n",
    "\n",
    "# axes = df_diag.hist()\n",
    "# plt.title(\"Histogram of ship diagonal sizes\")\n",
    "# plt.xlabel(\"Number of ships\")\n",
    "# plt.ylabel(\"Diagonal in pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1afaa",
   "metadata": {},
   "source": [
    "**Dataset, Losses and Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006448d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDataset(Dataset):\n",
    "    def __init__(self, in_df, transform=None, mode='train'):\n",
    "        grp = list(in_df.groupby('ImageId'))\n",
    "        self.image_ids =  [_id for _id, _ in grp] \n",
    "        self.image_masks = [m['EncodedPixels'].values for _,m in grp]\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.img_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])  # use mean and std from ImageNet \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "               \n",
    "    def __getitem__(self, idx):\n",
    "        img_file_name = self.image_ids[idx]\n",
    "        if (self.mode == 'train') | (self.mode == 'validation'):\n",
    "            rgb_path = os.path.join(path_train, img_file_name)\n",
    "        else:\n",
    "            rgb_path = os.path.join(path_test, img_file_name)\n",
    "        img = imread(rgb_path)\n",
    "        mask = masks_as_image(self.image_masks[idx])\n",
    "        \n",
    "        if self.transform is not None: \n",
    "            img, mask = self.transform(img, mask)\n",
    "            \n",
    "        if (self.mode == 'train') | (self.mode == 'validation'):\n",
    "            return self.img_transform(img), torch.from_numpy(np.moveaxis(mask, -1, 0)).float()  \n",
    "        else:\n",
    "            return self.img_transform(img), str(img_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3263889",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = list(train_df.groupby('ImageId'))\n",
    "image_ids =  [_id for _id, _ in grp]\n",
    "image_masks = [m['EncodedPixels'].values for _,m in grp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cea46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_masks), len(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks_as_image(image_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1049f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape, plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f38283",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "img_file_name = image_ids[0]\n",
    "rgb_path = os.path.join(path_train, img_file_name)\n",
    "img = imread(rgb_path)\n",
    "img_transform(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.moveaxis(mask, -1, 0)).float().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a69c0a",
   "metadata": {},
   "source": [
    "**Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(img, dtype, maxval):\n",
    "    return np.clip(img, 0, maxval).astype(dtype)\n",
    "\n",
    "class DualCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        for t in self.transforms:\n",
    "            x, mask = t(x, mask)\n",
    "        return x, mask\n",
    "\n",
    "class ImageOnly:\n",
    "    def __init__(self, trans):\n",
    "        self.trans = trans\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        return self.trans(x), mask\n",
    "\n",
    "\n",
    "class VerticalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        if random.random() < self.prob:\n",
    "            img = cv2.flip(img, 0)\n",
    "            if mask is not None:\n",
    "                mask = cv2.flip(mask, 0)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class HorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        if random.random() < self.prob:\n",
    "            img = cv2.flip(img, 1)\n",
    "            if mask is not None:\n",
    "                mask = cv2.flip(mask, 1)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class RandomFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        if random.random() < self.prob:\n",
    "            d = random.randint(-1, 1)\n",
    "            img = cv2.flip(img, d)\n",
    "            if mask is not None:\n",
    "                mask = cv2.flip(mask, d)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class Rotate:\n",
    "    def __init__(self, limit=90, prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.limit = limit\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        if random.random() < self.prob:\n",
    "            angle = random.uniform(-self.limit, self.limit)\n",
    "\n",
    "            height, width = img.shape[0:2]\n",
    "            mat = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1.0)\n",
    "            img = cv2.warpAffine(img, mat, (height, width),\n",
    "                                 flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_REFLECT_101)\n",
    "            if mask is not None:\n",
    "                mask = cv2.warpAffine(mask, mat, (height, width),\n",
    "                                      flags=cv2.INTER_LINEAR,\n",
    "                                      borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size):\n",
    "        self.h = size[0]\n",
    "        self.w = size[1]\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        h_start = np.random.randint(0, height - self.h)\n",
    "        w_start = np.random.randint(0, width - self.w)\n",
    "\n",
    "        img = img[h_start: h_start + self.h, w_start: w_start + self.w,:]\n",
    "\n",
    "        assert img.shape[0] == self.h\n",
    "        assert img.shape[1] == self.w\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.ndim == 2:\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "            mask = mask[h_start: h_start + self.h, w_start: w_start + self.w,:]\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size):\n",
    "        self.height = size[0]\n",
    "        self.width = size[1]\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        h, w, c = img.shape\n",
    "        dy = (h - self.height) // 2\n",
    "        dx = (w - self.width) // 2\n",
    "        y1 = dy\n",
    "        y2 = y1 + self.height\n",
    "        x1 = dx\n",
    "        x2 = x1 + self.width\n",
    "        img = img[y1:y2, x1:x2,:]\n",
    "        if mask is not None:\n",
    "            if mask.ndim == 2:\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "            mask = mask[y1:y2, x1:x2,:]\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaeb8f",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, true, batch_size=16, threshold=0.5):\n",
    "    pred = pred.view(batch_size, -1)\n",
    "    true = true.view(batch_size, -1)\n",
    "    \n",
    "    pred = (pred > threshold).float()\n",
    "    true = (true > threshold).float()\n",
    "    \n",
    "    pred_sum = pred.sum(-1)\n",
    "    true_sum = true.sum(-1)\n",
    "    \n",
    "    neg_index = torch.nonzero(true_sum == 0)\n",
    "    pos_index = torch.nonzero(true_sum >= 1)\n",
    "    \n",
    "    dice_neg = (pred_sum == 0).float()\n",
    "    dice_pos = 2 * ((pred * true).sum(-1)) / ((pred + true).sum(-1))\n",
    "    \n",
    "    dice_neg = dice_neg[neg_index]\n",
    "    dice_pos = dice_pos[pos_index]\n",
    "    \n",
    "    dice = torch.cat([dice_pos, dice_neg])\n",
    "    jaccard = dice / (2 - dice)\n",
    "    \n",
    "    return dice.numpy(), jaccard.numpy()\n",
    "\n",
    "class metrics:\n",
    "    def __init__(self, batch_size=16, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.batchsize = batch_size\n",
    "        self.dice = []\n",
    "        self.jaccard = []\n",
    "    def collect(self, pred, true):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        dice, jaccard = compute_metrics(pred, true, batch_size=self.batchsize, threshold=self.threshold)\n",
    "        self.dice.extend(dice)\n",
    "        self.jaccard.extend(jaccard)\n",
    "    def get(self):\n",
    "        #print(self.dice.shape, self.dice)\n",
    "        dice = np.nanmean(self.dice)\n",
    "        jaccard = np.nanmean(self.jaccard)\n",
    "        return dice, jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb281b8",
   "metadata": {},
   "source": [
    "**Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b92df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEJaccardWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, jaccard_weight=1, smooth=1):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.jaccard_weight = jaccard_weight\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        if outputs.size() != targets.size():\n",
    "            raise ValueError(\"size mismatch, {} != {}\".format(outputs.size(), targets.size()))\n",
    "            \n",
    "        loss = self.bce(outputs, targets)\n",
    "\n",
    "        if self.jaccard_weight:\n",
    "            targets = (targets == 1.0).float()\n",
    "            targets = targets.view(-1)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            outputs = outputs.view(-1)\n",
    "\n",
    "            intersection = (targets * outputs).sum()\n",
    "            union = outputs.sum() + targets.sum() - intersection\n",
    "\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + self.smooth ) / (union + self.smooth )) # try with 1-dice\n",
    "        return loss\n",
    "\n",
    "class BCEDiceWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=1, smooth=1):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def __call__(self, outputs, targets):\n",
    "        if outputs.size() != targets.size():\n",
    "            raise ValueError(\"size mismatch, {} != {}\".format(outputs.size(), targets.size()))\n",
    "            \n",
    "        loss = self.bce(outputs, targets)\n",
    "\n",
    "        targets = (targets == 1.0).float()\n",
    "        targets = targets.view(-1)\n",
    "        outputs = F.sigmoid(outputs)\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        intersection = (outputs * targets).sum()\n",
    "        dice = 2.0 * (intersection + self.smooth)  / (targets.sum() + outputs.sum() + self.smooth)\n",
    "        \n",
    "        loss -= self.dice_weight * torch.log(dice) # try with 1- dice\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class FocalWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, outputs, targets):\n",
    "        if outputs.size() != targets.size():\n",
    "            raise ValueError(\"size mismatch, {} != {}\".format(outputs.size(), targets.size()))\n",
    "            \n",
    "        loss = self.bce(outputs, targets)\n",
    "\n",
    "        targets = (targets == 1.0).float()\n",
    "        targets = targets.view(-1)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        outputs = outputs.view(-1)\n",
    "        outputs = torch.where(targets == 1, outputs, 1 - outputs)\n",
    "\n",
    "        focal = self.alpha * (1 - outputs) ** (self.gamma)\n",
    "        loss *= focal.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_loss_actual(input, target):\n",
    "    num = target.size(0)\n",
    "    inputs = inputs.reshape(num, -1)\n",
    "    target = target.reshape(num, -1)\n",
    "    smooth = 1.0\n",
    "    intersection = (inputs * target)\n",
    "    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
    "    dice = 1 - dice.sum() / num\n",
    "    return dice\n",
    "\n",
    "def bce_dice_loss_actual(inputs, target):\n",
    "    dicescore = dice_loss(inputs, target)\n",
    "    bcescore = nn.BCELoss()\n",
    "    bceloss = bcescore(inputs, target)\n",
    "\n",
    "    return bceloss + dicescore\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60676d11",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IUNet_down_block(torch.nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, down_size):\n",
    "        super(IUNet_down_block, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(input_channel, output_channel, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.down_size = down_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.down_size:\n",
    "            x = self.max_pool(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "class IUNet_up_block(torch.nn.Module):\n",
    "    def __init__(self, prev_channel, input_channel, output_channel):\n",
    "        super(IUNet_up_block, self).__init__()\n",
    "        self.up_sampling = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv1 = torch.nn.Conv2d(prev_channel + input_channel, output_channel, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, prev_feature_map, x):\n",
    "        x = self.up_sampling(x)\n",
    "        x = torch.cat((x, prev_feature_map), dim=1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class IUNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IUNet, self).__init__()\n",
    "\n",
    "        self.down_block1 = IUNet_down_block(3, 16, False)\n",
    "        self.down_block2 = IUNet_down_block(16, 32, True)\n",
    "        self.down_block3 = IUNet_down_block(32, 64, True)\n",
    "        self.down_block4 = IUNet_down_block(64, 128, True)\n",
    "        self.down_block5 = IUNet_down_block(128, 256, True)\n",
    "        self.down_block6 = IUNet_down_block(256, 512, True)\n",
    "        self.down_block7 = IUNet_down_block(512, 1024, True)\n",
    "\n",
    "        self.mid_conv1 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(1024)\n",
    "        self.mid_conv2 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(1024)\n",
    "        self.mid_conv3 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.up_block1 = IUNet_up_block(512, 1024, 512)\n",
    "        self.up_block2 = IUNet_up_block(256, 512, 256)\n",
    "        self.up_block3 = IUNet_up_block(128, 256, 128)\n",
    "        self.up_block4 = IUNet_up_block(64, 128, 64)\n",
    "        self.up_block5 = IUNet_up_block(32, 64, 32)\n",
    "        self.up_block6 = IUNet_up_block(16, 32, 16)\n",
    "\n",
    "        self.last_conv1 = torch.nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.last_bn = torch.nn.BatchNorm2d(16)\n",
    "        self.last_conv2 = torch.nn.Conv2d(16, 1, 1, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.down_block1(x)\n",
    "        self.x2 = self.down_block2(self.x1)\n",
    "        self.x3 = self.down_block3(self.x2)\n",
    "        self.x4 = self.down_block4(self.x3)\n",
    "        self.x5 = self.down_block5(self.x4)\n",
    "        self.x6 = self.down_block6(self.x5)\n",
    "        self.x7 = self.down_block7(self.x6)\n",
    "        self.x7 = self.relu(self.bn1(self.mid_conv1(self.x7)))\n",
    "        self.x7 = self.relu(self.bn2(self.mid_conv2(self.x7)))\n",
    "        self.x7 = self.relu(self.bn3(self.mid_conv3(self.x7)))\n",
    "        x = self.up_block1(self.x6, self.x7)\n",
    "        x = self.up_block2(self.x5, x)\n",
    "        x = self.up_block3(self.x4, x)\n",
    "        x = self.up_block4(self.x3, x)\n",
    "        x = self.up_block5(self.x2, x)\n",
    "        x = self.up_block6(self.x1, x)\n",
    "        x = self.relu(self.last_bn(self.last_conv1(x)))\n",
    "        x = self.last_conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883a643",
   "metadata": {},
   "source": [
    "**My Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27058d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageSegmentation(num_classes):\n",
    "    PRE_TRAINED_NET = torchvision.models.resnet18(pretrained=True)\n",
    "    model = nn.Sequential(*list(PRE_TRAINED_NET.children())[:-2])\n",
    "    model.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "    model.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "    W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "    model.transpose_conv.weight.data.copy_(W)\n",
    "\n",
    "    return model\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),torch.arange(kernel_size).reshape(1, -1))\n",
    "    \n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "    kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62145e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ImageSegmentation(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74375c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = DualCompose([HorizontalFlip(), VerticalFlip(), RandomCrop((256,256,3))])\n",
    "val_transform = DualCompose([CenterCrop((512,512,3))])\n",
    "\n",
    "train_dataset = AirbusDataset(train_df[:2100], transform=train_transform, mode='train')\n",
    "val_dataset = AirbusDataset(valid_df[:200], transform=val_transform, mode='validation')\n",
    "\n",
    "print('Train samples : %d | Validation samples : %d' % (len(train_dataset), len(val_dataset)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SZ_TRAIN, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SZ_VALID, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_b, labels_b = next(iter(train_loader))\n",
    "images_b.shape, labels_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if SHOW_IMG_LOADER == True:\n",
    "#     # Display some images from loader\n",
    "# images, mask = next(iter(train_loader))\n",
    "# imshow_mask(torchvision.utils.make_grid(images, nrow=1), torchvision.utils.make_grid(mask, nrow=1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "run_id = 1\n",
    "\n",
    "def loss_fn(LOSS):   \n",
    "    if LOSS == 'BCEWithDigits':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif LOSS == 'FocalLossWithDigits':\n",
    "        criterion = MixedLoss(10, 2)\n",
    "    elif LOSS == 'BCEDiceWithLogitsLoss':\n",
    "        criterion = BCEDiceWithLogitsLoss()\n",
    "    elif LOSS == 'BCEJaccardWithLogitsLoss':\n",
    "        criterion = BCEJaccardWithLogitsLoss()\n",
    "    else:\n",
    "        raise NameError(\"loss not supported\")\n",
    "    \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, model, criterion, train_loader, valid_loader, init_optimizer, train_batch_sz=16, valid_batch_sz=4, n_epochs=1, fold=1):\n",
    "    \n",
    "    model_path = Path('model_{fold}.pt'.format(fold=fold))\n",
    "    if model_path.exists():\n",
    "        state = torch.load(str(model_path))\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        model.load_state_dict(state['model'])\n",
    "        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n",
    "    else:\n",
    "        epoch = 1\n",
    "        step = 0\n",
    "\n",
    "    save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "    }, str(model_path))\n",
    "\n",
    "    report_each = 50\n",
    "    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = init_optimizer(lr)\n",
    "\n",
    "    for epoch in range(epoch, n_epochs + 1):\n",
    "        model.train()\n",
    "        random.seed()\n",
    "        tq = tqdm(total=len(train_loader) *  train_batch_sz)\n",
    "        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n",
    "        losses = []\n",
    "        valid_metrics = metrics(batch_size=valid_batch_sz)  # for validation\n",
    "        tl = train_loader\n",
    "        try:\n",
    "            mean_loss = 0\n",
    "            for i, (inputs, targets) in enumerate(tl):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                batch_size = inputs.size(0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "                tq.update(batch_size)\n",
    "                losses.append(loss.item())\n",
    "                mean_loss = np.mean(losses[-report_each:])\n",
    "                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n",
    "                if i and i % report_each == 0:\n",
    "                    write_event(log, step, loss=mean_loss)\n",
    "            write_event(log, step, loss=mean_loss)\n",
    "            tq.close()\n",
    "            save(epoch + 1)\n",
    "            \n",
    "            # Validation\n",
    "            comb_loss_metrics = validation(model, criterion, valid_loader, valid_metrics)\n",
    "            write_event(log, step, **comb_loss_metrics)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            tq.close()\n",
    "            print('Ctrl+C, saving snapshot')\n",
    "            save(epoch)\n",
    "            print('done.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model: nn.Module, criterion, valid_loader, metrics):\n",
    "    print(\"Validation\")\n",
    "    \n",
    "    losses = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs, targets in valid_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        metrics.collect(outputs.detach().cpu(), targets.detach().cpu()) # get metrics \n",
    "    \n",
    "    valid_loss = np.mean(losses)  # float\n",
    "    valid_dice, valid_jaccard = metrics.get() # float\n",
    "\n",
    "    print('Valid loss: {:.5f}, Jaccard: {:.5f}, Dice: {:.5f}'.format(valid_loss, valid_jaccard, valid_dice))\n",
    "    comb_loss_metrics = {'valid_loss': valid_loss, 'jaccard': valid_jaccard.item(), 'dice': valid_dice.item()}\n",
    "    \n",
    "    #comb_loss_metrics = {'valid_loss': valid_loss, 'jaccard': valid_jaccard, 'dice': valid_dice}\n",
    "    \n",
    "    return comb_loss_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_event(log, step: int, **data):\n",
    "    data['step'] = step\n",
    "    data['dt'] = datetime.now().isoformat()\n",
    "    log.write(json.dumps(data, sort_keys=True))\n",
    "    log.write('\\n')\n",
    "    log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00db15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch_sz=4\n",
    "valid_metrics = metrics(batch_size=valid_batch_sz)\n",
    "val = validation(net, loss_fn('BCEDiceWithLogitsLoss'), val_loader, valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(init_optimizer=lambda lr: optim.Adam(net.parameters(), lr=lr),\n",
    "        lr = LR,\n",
    "        n_epochs = N_EPOCHS,\n",
    "        model=net,\n",
    "        criterion=loss_fn('BCEDiceWithLogitsLoss'),\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=val_loader,\n",
    "        train_batch_sz= BATCH_SZ_TRAIN,\n",
    "        valid_batch_sz=BATCH_SZ_VALID,\n",
    "        fold=run_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983129fc",
   "metadata": {},
   "source": [
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "path = r'D:\\PyTorch\\Airbus_Image_Segmentation\\airbus-ship-detection\\models\\model_1.pt'\n",
    "imagePath = r'D:\\PyTorch\\Airbus_Image_Segmentation\\test_airbus\\00e90efc3.jpg'\n",
    "checkpoint = torch.load(path)\n",
    "net = ImageSegmentation(1)\n",
    "net.load_state_dict(checkpoint['model'])\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net = net.eval()\n",
    "net = net.to('cuda')\n",
    "img = imread(imagePath)\n",
    "img_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "img = img_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.cuda()\n",
    "import cv2\n",
    "pred = net(img)\n",
    "k = torch.permute(pred, (0, 2, 3, 1))\n",
    "k = torch.squeeze(k)\n",
    "k = k.cpu().detach().numpy() \n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(k) # cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be73d6",
   "metadata": {},
   "source": [
    "**Dice Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d832ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9267c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_b, labels_b = next(iter(train_loader))\n",
    "images_b.shape, labels_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b599a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss(outputs, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ea095",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.sigmoid(outputs)\n",
    "smooth = 1.0\n",
    "iflat = input.view(-1)\n",
    "tflat = labels_b.view(-1)\n",
    "intersection = (iflat * tflat).sum()\n",
    "\n",
    "loss = ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iflat), len(tflat), intersection, input.shape, 1-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = labels_b\n",
    "inputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = target.size(0)\n",
    "inputs = F.sigmoid(inputs)\n",
    "inputs = inputs.reshape(num, -1)\n",
    "target = target.reshape(num, -1)\n",
    "smooth = 1.0\n",
    "intersection = (inputs * target)\n",
    "dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
    "dice = 1 - dice.sum() / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d17745",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice, num, inputs.shape, target.shape, intersection.sum(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6af60",
   "metadata": {},
   "source": [
    "**Valid dice loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Loss\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss\n",
    "#PyTorch\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = (inputs * targets).sum()    \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "       \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        \n",
    "        return 1 - Tversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "GAMMA = 1\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = (inputs * targets).sum()    \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "        \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = (1 - Tversky)**gamma\n",
    "                       \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class LovaszHingeLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(LovaszHingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = F.sigmoid(inputs)    \n",
    "        Lovasz = lovasz_hinge(inputs, targets, per_image=False)                       \n",
    "        return Lovasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n",
    "CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(ComboLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9):\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        intersection = (inputs * targets).sum()    \n",
    "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        inputs = torch.clamp(inputs, eps, 1.0 - eps)       \n",
    "        out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n",
    "        weighted_ce = out.mean(-1)\n",
    "        combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n",
    "        \n",
    "        return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d97366",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = TverskyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734866ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(inputs, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c307",
   "metadata": {},
   "source": [
    "**Multi class classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "inputs = torch.randn(32, 5, 128, 128)\n",
    "targets = torch.randint(0, 5, (32, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_onehot_labels(labels, label_weights, target_shape, ignore_index):\n",
    "    \"\"\"Expand onehot labels to match the size of prediction.\"\"\"\n",
    "    bin_labels = labels.new_zeros(target_shape)\n",
    "    valid_mask = (labels >= 0) & (labels != ignore_index)\n",
    "    inds = torch.nonzero(valid_mask, as_tuple=True)\n",
    "\n",
    "    if inds[0].numel() > 0:\n",
    "        if labels.dim() == 3:\n",
    "            bin_labels[inds[0], labels[valid_mask], inds[1], inds[2]] = 1\n",
    "        else:\n",
    "            bin_labels[inds[0], labels[valid_mask]] = 1\n",
    "\n",
    "    valid_mask = valid_mask.unsqueeze(1).expand(target_shape).float()\n",
    "    if label_weights is None:\n",
    "        bin_label_weights = valid_mask\n",
    "    else:\n",
    "        bin_label_weights = label_weights.unsqueeze(1).expand(target_shape)\n",
    "        bin_label_weights *= valid_mask\n",
    "\n",
    "    return bin_labels, bin_label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_index = -100 #if ignore_index is None else ignore_index\n",
    "if inputs.dim() != targets.dim():\n",
    "    assert (inputs.dim() == 2 and targets.dim() == 1) or (inputs.dim() == 4 and targets.dim() == 3), \\\n",
    "    'Only pred shape [N, C], label shape [N] or pred shape [N, C, H, W], label shape [N, H, W] are supported'\n",
    "\n",
    "    label, weight = _expand_onehot_labels(targets, inputs, inputs.shape, ignore_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_cross_entropy(pred,\n",
    "                       target,\n",
    "                       label,\n",
    "                       reduction='mean',\n",
    "                       avg_factor=None,\n",
    "                       class_weight=None,\n",
    "                       ignore_index=None,\n",
    "                       **kwargs):\n",
    "    \"\"\"Calculate the CrossEntropy loss for masks.\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C, *), C is the\n",
    "            number of classes. The trailing * indicates arbitrary shape.\n",
    "        target (torch.Tensor): The learning label of the prediction.\n",
    "        label (torch.Tensor): ``label`` indicates the class label of the mask\n",
    "            corresponding object. This will be used to select the mask in the\n",
    "            of the class which the object belongs to when the mask prediction\n",
    "            if not class-agnostic.\n",
    "        reduction (str, optional): The method used to reduce the loss.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (list[float], optional): The weight for each class.\n",
    "        ignore_index (None): Placeholder, to be consistent with other loss.\n",
    "            Default: None.\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    Example:\n",
    "        >>> N, C = 3, 11\n",
    "        >>> H, W = 2, 2\n",
    "        >>> pred = torch.randn(N, C, H, W) * 1000\n",
    "        >>> target = torch.rand(N, H, W)\n",
    "        >>> label = torch.randint(0, C, size=(N,))\n",
    "        >>> reduction = 'mean'\n",
    "        >>> avg_factor = None\n",
    "        >>> class_weights = None\n",
    "        >>> loss = mask_cross_entropy(pred, target, label, reduction,\n",
    "        >>>                           avg_factor, class_weights)\n",
    "        >>> assert loss.shape == (1,)\n",
    "    \"\"\"\n",
    "    assert ignore_index is None, 'BCE loss does not support ignore_index'\n",
    "    # TODO: handle these two reserved arguments\n",
    "    assert reduction == 'mean' and avg_factor is None\n",
    "    num_rois = pred.size()[0]\n",
    "    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n",
    "    pred_slice = pred[inds, label].squeeze(1)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        pred_slice, target, weight=class_weight, reduction='mean')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C = 16, 11\n",
    "H, W = 128, 128\n",
    "pred = torch.randn(N, C, H, W) * 1000\n",
    "target = torch.randn(N, H, W)\n",
    "#target = torch.randint(0, C, (N, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.randint(0, C, size=(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 'mean'\n",
    "avg_factor = None\n",
    "class_weights = None\n",
    "loss = mask_cross_entropy(pred, target, label, reduction, avg_factor, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rois = target.size()[0]\n",
    "num_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e283927",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c36826",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_slice = pred[inds, label].squeeze(1)\n",
    "pred_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5bc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (image_seg)",
   "language": "python",
   "name": "image_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
